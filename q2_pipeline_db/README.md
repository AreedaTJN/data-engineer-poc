# การจัดการข้อมูล, สร้าง Pipeline และสร้างฐานข้อมูล
q2_pipeline_db ประกอบการทำงานด้วย 3 สคริปต์ คือ การสร้าง Class ตามรูปแบบ Object-Oriented Programming (OOP), สร้างฐานข้อมูลด้วย PostgreSQL (pgAdmin 4) และการสร้าง DAG ของ Apache Airflow (โฟลเดอร์ airflow-docker)
## 1. สร้าง Class เพื่อการจัดการคุณภาพของข้อมูล (class_data_processor.py)
นำไฟล์ข้อมูลที่ได้จาก 1.2 การประมวลผลข้อมูลจากไฟล์ Excel/CSV มาประมวลผลต่อ โดยในไฟล์ class_data_processor.py จะประกอบด้วยฟังก์ชันดังนี้
- standardize_columns: เปลี่ยนชื่อคอลัมน์ให้เป็นแบบเดียวกันของแต่ละแหล่งข้อมูลที่มีแต่ชื่อคอลัมน์ต่างกัน
- handle_missing_values: จัดการข้อมูลที่เป็นค่าว่างด้วยการแทนที่ค่าเป็น "ไม่ระบุ"
- deduplicate: ลบข้อมูลที่มีการซ้ำกันของข้อมูล
- process: จัดการข้อมูลให้สะอาดโดยใช้ฟังก์ชันข้างต้น
- combine: รวมข้อมูลของแต่ละแหล่งเป็นไฟล์เดียว เพื่อนำเข้าสู่ฐานข้อมูล (combined_data.csv)
- test_class.py: เรียกใช้ฟังก์ชันที่สร้างพร้อมแปลง SOURCE เป็นค่า 1, 2, 3 เพื่อนำเข้าฐานข้อมูล และเปลี่ยนชื่อคอลัมน์ให้เป็นตัวพิมพ์เล็กทั้งหมด เพื่อให้สอดคล้องกับฐานข้อมูลที่สร้างขึ้น สุดท้ายจะส่งออกข้อมูลที่เป็นไฟล์ combined_data.csv
## 2. สร้างฐานข้อมูลด้วย PostgreSQL (pgAdmin 4) (csv_to_db.py)
นำข้อมูลไฟล์ csv (combined_data.csv) เข้าสู่ฐานข้อมูล โดยใช้ library psycopg2
- สร้างฟังก์ชันสำหรับการโหลดข้อมูล csv เข้าสู่ฐานข้อมูล

# หมายเหตุ
1. ไม่ได้ใช้ airflow เนื่องจากติดปัญหาการเรียกใช้ฟังก์ชัน (airflow_docker) จึงใช้วิธีการนำเข้าข้อมูลแบบเขียนโค้ด Python
2. ต้องสร้างฐานข้อมูลและตารางข้อมูล (create_tables.sql) ในฐานข้อมูลให้เรียบร้อยก่อนการนำ csv เข้าฐานข้อมูล และจำเป็นต้องให้สิทธิ์การเข้าถึงให้เรียบร้อย


